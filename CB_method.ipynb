{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shao.367/miniconda3/envs/pt-gpu/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model import Model\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open('data/dimension.pickle', 'rb') as pk:\n",
    "    dim_info = pickle.load(pk)\n",
    "Sl_d, Sf_d, A_d = 1, dim_info['tages'], dim_info['tages']\n",
    "T = 100\n",
    "\n",
    "SCALE = 0.36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2904.0760046376627, 2904.0759972004635, 2904.0876477176216, 2904.1599326471433, 2904.105476801149, 2904.0802437570183, 2904.1840263636254, 2904.0900433453767, 2904.313741831054, 2904.0884309906637]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shao.367/miniconda3/envs/pt-gpu/lib/python3.9/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator KMeans from version 1.1.3 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_137206/188901844.py:26: RuntimeWarning: Mean of empty slice.\n",
      "  y_current = np.array(w_y).mean(axis=0)\n",
      "/home/shao.367/miniconda3/envs/pt-gpu/lib/python3.9/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "all_rewards = []\n",
    "\n",
    "for user_class in range(10):\n",
    "    model = Model(Sl_d, Sf_d, A_d, T, bias=0.3, normal_scale=SCALE)\n",
    "    state_slow, state_fast = model.reset(user_class)\n",
    "    rewards, actions, states_slow, states_fast = [], [], [], []\n",
    "    A_prob = np.random.uniform(0, 1, size=len(state_fast))\n",
    "    action_vec = np.zeros(len(A_prob))\n",
    "    action = np.random.choice(len(A_prob), 1, p=A_prob/A_prob.sum()) \n",
    "    action_vec[action] = 1\n",
    "\n",
    "    WINDOW_SIZE = 5\n",
    "\n",
    "    while True:\n",
    "        states_slow.append(state_slow)\n",
    "        states_fast.append(state_fast)\n",
    "        actions.append(action)\n",
    "\n",
    "        state_slow, state_fast, terminal = model.update(state_slow, state_fast, action_vec, user_class)\n",
    "        reward = model.reward(state_slow, state_fast, action_vec)\n",
    "        \n",
    "        if len(states_fast) >= WINDOW_SIZE:\n",
    "            w_y = states_fast[:-WINDOW_SIZE] \n",
    "        else:\n",
    "            w_y = states_fast\n",
    "        y_current = np.array(w_y).mean(axis=0)\n",
    "\n",
    "\n",
    "        action_vec = np.zeros(len(state_fast))\n",
    "        if state_fast.sum() > 0:\n",
    "            action = np.random.choice(len(action_vec), 1, p=state_fast/state_fast.sum())\n",
    "        else:\n",
    "            action = np.random.choice(len(action_vec), 1)\n",
    "        action_vec[action] = 1\n",
    "        rewards.append(reward)\n",
    "\n",
    "        if terminal:\n",
    "            all_rewards.append(np.array(rewards).sum())\n",
    "            break\n",
    "    \n",
    "print(all_rewards)\n",
    "\n",
    "with open(f'data/CB_rewards.pickle', 'wb') as pk:\n",
    "    pickle.dump(all_rewards, pk, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1509.689778645044, 514.9562258732418, 1311.5764850182343, 112.92995911615678, 1610.4885639620995, 2407.7521114567226, 1509.953136665449, 1013.2184049060061, 1810.958875061906, 1710.7905330480926]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_114552/2482508208.py:29: RuntimeWarning: Mean of empty slice.\n",
      "  y_current = np.array(w_y).mean(axis=0)\n"
     ]
    }
   ],
   "source": [
    "heuristic_rewards = []\n",
    "\n",
    "for user_class in range(10):\n",
    "    model = Model(Sl_d, Sf_d, A_d, T, bias=0.3, normal_scale=SCALE)\n",
    "    state_slow, state_fast = model.reset(user_class)\n",
    "    rewards, actions, states_slow, states_fast = [], [], [], []\n",
    "    A_prob = np.random.uniform(0, 1, size=len(state_fast))\n",
    "    action_vec = np.zeros(len(A_prob))\n",
    "    action = np.random.choice(len(A_prob), 1, p=A_prob/A_prob.sum()) \n",
    "    action_vec[action] = 1\n",
    "\n",
    "    WINDOW_SIZE = 5\n",
    "    UPDATE_WINDOWN = 10\n",
    "\n",
    "    cnt = 0\n",
    "    while True:\n",
    "        \n",
    "        states_slow.append(state_slow)\n",
    "        states_fast.append(state_fast)\n",
    "        actions.append(action)\n",
    "\n",
    "        state_slow, state_fast, terminal = model.update(state_slow, state_fast, action_vec, user_class)\n",
    "        reward = model.reward(state_slow, state_fast, action_vec)\n",
    "        \n",
    "        if len(states_fast) >= WINDOW_SIZE:\n",
    "            w_y = states_fast[:-WINDOW_SIZE] \n",
    "        else:\n",
    "            w_y = states_fast\n",
    "        y_current = np.array(w_y).mean(axis=0)\n",
    "\n",
    "        if cnt % UPDATE_WINDOWN == 0 and cnt != 0:\n",
    "            action_vec = np.zeros(len(state_fast))\n",
    "            if state_fast.sum() > 0:\n",
    "                action = np.random.choice(len(action_vec), 1, p=state_fast/state_fast.sum())\n",
    "            else:\n",
    "                action = np.random.choice(len(action_vec), 1)\n",
    "            action_vec[action] = 1\n",
    "        rewards.append(reward)\n",
    "\n",
    "        if terminal:\n",
    "            heuristic_rewards.append(np.array(rewards).sum())\n",
    "            break\n",
    "        \n",
    "        cnt += 1\n",
    "    \n",
    "print(heuristic_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Aug 18 2021, 19:38:01) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ced00d027a5dbfd53bf9a418350af530543228d182f68e857f8b219fbe4842d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
